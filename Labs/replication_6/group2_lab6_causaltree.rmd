---
title: "group2_lab6"
author: "Diego Alonso Gómez (20171738) & Alexander Pacheco (20161901)"
date: "18/11/2021"
output: html_document
---


# 1° Step: How the tree was built?

```{r}
set.seed(1)

rm(list = ls())

library(grf)
if(packageVersion("grf") < '0.10.2') {
  warning("This script requires grf 0.10.2 or higher")
}
library(sandwich)
library(lmtest)
library(Hmisc)
library(ggplot2)

data.all = read.csv("D:\\Escritorio\\Trabajo Estadistica Aplicada\\data\\synthetic_data.csv")
data.all$schoolid = factor(data.all$schoolid)

DF = data.all[,-1]
school.id = as.numeric(data.all$schoolid)

school.mat = model.matrix(~ schoolid + 0, data = data.all)
school.size = colSums(school.mat)

# It appears that school ID does not affect pscore. So ignore it
# in modeling, and just treat it as source of per-cluster error.
w.lm = glm(Z ~ ., data = data.all[,-3], family = binomial)
summary(w.lm)
```


```{r}
W = DF$Z
Y = DF$Y
X.raw = DF[,-(1:2)]

C1.exp = model.matrix(~ factor(X.raw$C1) + 0)
XC.exp = model.matrix(~ factor(X.raw$XC) + 0)

X = cbind(X.raw[,-which(names(X.raw) %in% c("C1", "XC"))], C1.exp, XC.exp)


# Grow a forest. Add extra trees for the causal forest.
#

Y.forest = regression_forest(X, Y, clusters = school.id, equalize.cluster.weights = TRUE)
Y.hat = predict(Y.forest)$predictions
W.forest = regression_forest(X, W, clusters = school.id, equalize.cluster.weights = TRUE)
W.hat = predict(W.forest)$predictions


# Aqui esta como se está costruyendo el aro¿bol: Se esta empleando la 
# la variable endogena Y (que signfica segun el Readme ....), el X(las covariables...)
# y el W (la variable dummy que representa el tratamiento) ..
cf.raw = causal_forest(X, Y, W,
                       Y.hat = Y.hat, W.hat = W.hat,
                       clusters = school.id,
                       equalize.cluster.weights = TRUE)
varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp))

cf = causal_forest(X[,selected.idx], Y, W,
                   Y.hat = Y.hat, W.hat = W.hat,
                   clusters = school.id,
                   equalize.cluster.weights = TRUE,
                   tune.parameters = "all")
tau.hat = predict(cf)$predictions
```

RPTA: Here we have that the tree was built with a machine learning algorithm known as decision tree learning to identify an optimal strategy for splitting observed individuals into groups to estimate heterogeneous treatment effects.

So, to build the tree, it was done the next steps:

1° As a first step we fit two  regression forest (Y.forest & W.forest) with all the other X features. That in order used them as inputs in the next random forest.

2° With the predictions obtained in W.hat, we estimate a pilot causal forest on all the features to keep the most important features in low-signal situations.

3° Next, with the important features obtained in cf.raw, we start to estimate the causal forest and save it in "cf", where we have that the tree was built with a machine learning algorithm known as decision tree learning to identify an optimal strategy for splitting observed individuals into groups to estimate heterogeneous treatment effects.


4°Finally, in tau.hat is shown each "tau" that represents each Contidional Treatment Effect found for the leafs of each tree's branch.  


# 2° Step: Estimate ATE

```{r}
ATE = average_treatment_effect(cf)
paste("95% CI for the ATE:", round(ATE[1], 3),
      "+/-", round(qnorm(0.975) * ATE[2], 3))

```

RPTA: Here we have that the ATE is 0.248 , with a +/- double standard deviation 0.039. Here we have that that ATE is found by finding the mean of the distribution of the heterogeneous effects that we have found, remember that from the sample we have constructed various sub-groups, and in each subgroup there is a conditional average treatment effect in each subgroup. So, the results establish that the intervetion treatment effect -related to the growth mindset- has a positive and significant impact on the measure of achievement of the students, that on average.

# 3° Step: Run best linear predictor analysis

```{r}
test_calibration(cf)

# Compare regions with high and low estimated CATEs ------
high_effect = tau.hat > median(tau.hat)
ate.high = average_treatment_effect(cf, subset = high_effect)
ate.low = average_treatment_effect(cf, subset = !high_effect)
paste("95% CI for difference in ATE:",
      round(ate.high[1] - ate.low[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))
#-------------------------------------------------------------
```

This test is used to prove whether the causal forest has succeeded in accurately estimating treatment heterogeneity or not.
# -------------------------------------------------------------------------
```{r}
# Compare regions with high and low estimated CATEs
high_effect = tau.hat > median(tau.hat)
ate.high = average_treatment_effect(cf, subset = high_effect)
ate.low = average_treatment_effect(cf, subset = !high_effect)
paste("95% CI for difference in ATE:",
      round(ate.high[1] - ate.low[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))

```

This test is used to prove whether the causal forest has succeeded in accurately estimating treatment heterogeneity or not. Therefore, we can see that the coefficient on $D_i$ is positive and significant, so we can't not deny correlation between the CATE and the real effect of the treatment: it shows that there's almost no treatment heterogenity, so causal forest couldn't identify subgroups with effects at the end.

Having noticed this, the authors try to found if there's heterogenity between $X_1$ ( pre-existing mindset) and $X_2$ (school-level achievement). They find that schools with larger values of $X_1$ have larger effects than schools with smaller values of this variables. However, they do not see much heterogeneity along $X_2$.

```{r}
#
# formal test for X1 and X2
#

dr.score = tau.hat + W / cf$W.hat *
  (Y - cf$Y.hat - (1 - cf$W.hat) * tau.hat) -
  (1 - W) / (1 - cf$W.hat) * (Y - cf$Y.hat + cf$W.hat * tau.hat)
school.score = t(school.mat) %*% dr.score / school.size

school.X1 = t(school.mat) %*% X$X1 / school.size
high.X1 = school.X1 > median(school.X1)
t.test(school.score[high.X1], school.score[!high.X1])

school.X2 = (t(school.mat) %*% X$X2) / school.size
high.X2 = school.X2 > median(school.X2)
t.test(school.score[high.X2], school.score[!high.X2])

school.X2.levels = cut(school.X2,
  breaks = c(-Inf, quantile(school.X2, c(1/3, 2/3)), Inf))
summary(aov(school.score ~ school.X2.levels))

#
# formal test for S3
#

school.score.XS3.high = t(school.mat) %*% (dr.score * (X$S3 >= 6)) /
  t(school.mat) %*% (X$S3 >= 6)
school.score.XS3.low = t(school.mat) %*% (dr.score * (X$S3 < 6)) /
  t(school.mat) %*% (X$S3 < 6)

plot(school.score.XS3.low, school.score.XS3.high)
t.test(school.score.XS3.high - school.score.XS3.low)
```
# -------------------------------------------------------------------------


# 4° Step: Look at school-wise heterogeneity

```{r}
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(school.score, xlab = "School Treatment Effect Estimate", main = "")
```

RPTA: This distribution graph showed is the distribution of the heterogeneous effects that we have found

But let us remember that from the sample we have constructed various subgroups, and in each subgroup there is a conditional average treatment effect in each subgroup. This graph is the distribution of the "Conditional Average Treatment Effect" of each of the final leaves of the tree.

So, we can see that the distribution does not have a symmetric and centered shape in the ATE, but has a right tail that is much longer than the one on the left.

Furthermore, we can observe that the CATE distribution is almost centered on values greater than 0, so we can say that the ATE coefficient is statistically significant, at least at 5%. That because there is a decent variability in the distibution of school treatment effect estimates, that goes between -0.2 and 0.8. Althought some values are negative, most of the distribution seem to be on the positive side of axis.


# -------------------------------------------------------------------------
```{r}
# Re-check ATE... sanity check only
#

ate.hat = mean(school.score)
se.hat = sqrt(var(school.score) / length(school.score - 1))
print(paste(round(ate.hat, 3), "+/-", round(1.96 * se.hat, 3)))


#
# Look at variation in propensity scores
#

DF = X
DF$W.hat = cf$W.hat

#pscore.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
boxplot(W.hat ~ S3, data = DF, ylab = "Propensity Score", xlab = "Student Expectation of Success")
lines(smooth.spline(X$S3, cf$W.hat), lwd = 2, col = 4)

```


In this graphic we want to see what were the characteristics of that quartile, which had a small heterogeneous effect compared to those that had a higher heterogeneous effect. So, we can clearly see that Student Expectation of Success can explain the heterogeous effect of the treatment, especially for the range [5,7]. 


# -------------------------------------------------------------------------


# 5° Step: Analysis ignoring clusters. How do the results change?

```{r}

cf.noclust = causal_forest(X[,selected.idx], Y, W,
                           Y.hat = Y.hat, W.hat = W.hat,
                           tune.parameters = "all")

ATE.noclust = average_treatment_effect(cf.noclust)
paste("95% CI for the ATE:", round(ATE.noclust[1], 3),
      "+/-", round(qnorm(0.975) * ATE.noclust[2], 3))
```

```{r}
test_calibration(cf.noclust)
```

```{r}
tau.hat.noclust = predict(cf.noclust)$predict
plot(school.id, tau.hat.noclust)
```

```{r}
nfold = 5
school.levels = unique(school.id)
cluster.folds = sample.int(nfold, length(school.levels), replace = TRUE)

tau.hat.crossfold = rep(NA, length(Y))
for (foldid in 1:nfold) {
  print(foldid)
  infold = school.id %in% school.levels[cluster.folds == foldid]
  cf.fold = causal_forest(X[!infold, selected.idx], Y[!infold], W[!infold],
                          Y.hat = Y.hat[!infold], W.hat = W.hat[!infold],
                          tune.parameters = "all")
  pred.fold = predict(cf.fold, X[infold, selected.idx])$predictions
  tau.hat.crossfold[infold] = pred.fold
}

cf.noclust.cpy = cf.noclust
cf.noclust.cpy$predictions = tau.hat.crossfold
cf.noclust.cpy$clusters = school.id
test_calibration(cf.noclust.cpy)
```

```{r}
Rloss = mean(((Y - Y.hat) - tau.hat * (W - W.hat))^2)
Rloss.noclust = mean(((Y - Y.hat) - tau.hat.noclust * (W - W.hat))^2)
Rloss.crossfold = mean(((Y - Y.hat) - tau.hat.crossfold * (W - W.hat))^2)

c(Rloss.noclust - Rloss, Rloss.crossfold - Rloss)
```

```{r}
summary(aov(dr.score ~ factor(school.id)))
```

RPTA: In this summary we can see the heterogeneous effect of the treatment, wich was found by no considering the clusters in school banishes. So, the results tell us that the resultd have changed in comparison with the last step: it tell us that this methods are not safe since they are not robust to clustering, which should be considered. Therefore, we will keep the result found earlier: there's no enough evidence to assure us that there exist heteregonius effects


# 6° Step: Analysis without fitting the propensity score

```{r}
cf.noprop = causal_forest(X[,selected.idx], Y, W,
                          Y.hat = Y.hat, W.hat = mean(W),
                          tune.parameters = "all",
                          equalize.cluster.weights = TRUE,
                          clusters = school.id)
tau.hat.noprop = predict(cf.noprop)$predictions

ATE.noprop = average_treatment_effect(cf.noprop)
paste("95% CI for the ATE:", round(ATE.noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE.noprop[2], 3))
```

```{r}
#tauhat_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau.hat, tau.hat.noprop,
     xlim = range(tau.hat, tau.hat.noprop),
     ylim = range(tau.hat, tau.hat.noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
par = pardef

```

RPTA: In this graphic it can be seen that the propensity score is not as important compared with the results found at step 4; that because the ATE found here at step 6 (0.254) is very much similar to the one found using orthogalization. Precisely, this argument is shown in this last graphics, where the overlaped distributions are almost the same.


# -------------------------------------------------------------------------
```{r} 
# Train forest on school-wise DR scores
#

school.X = (t(school.mat) %*% as.matrix(X[,c(4:8, 25:28)])) / school.size
school.X = data.frame(school.X)
colnames(school.X) = c("X1", "X2", "X3", "X4", "X5",
                    "XC.1", "XC.2", "XC.3", "XC.4")

dr.score = tau.hat + W / cf$W.hat * (Y - cf$Y.hat - (1 - cf$W.hat) * tau.hat) -
  (1 - W) / (1 - cf$W.hat) * (Y - cf$Y.hat + cf$W.hat * tau.hat)
school.score = t(school.mat) %*% dr.score / school.size

school.forest = regression_forest(school.X, school.score)
school.pred = predict(school.forest)$predictions
test_calibration(school.forest)


# Alternative OLS analysis
school.DF = data.frame(school.X, school.score=school.score)
coeftest(lm(school.score ~ ., data = school.DF), vcov = vcovHC)
```
# -------------------------------------------------------------------------

# 7° Step: Six plots and their explanation so explain what you find there.

```{r}
#tauhat_hist.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau.hat, xlab = "estimated CATE", main = "")

```
Here is the distribution of the estimated CATE using clustering and orthogonzization. As we can see, the distribution is not very disperse (concentrates basically on 0.2 y 0.3) and it's no too large considering this values. This make us question whether there exist significant heteregenous effects, and, as seen before, there seems no to be heteregenous effects.


```{r}
#tauhat_hist_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau.hat.noprop, xlab = "estimated CATE", main = "")

```

```{r}
#tauhat_hist_noclust.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau.hat.noclust, xlab = "estimated CATE", main = "",
     breaks = seq(-0.0, 0.55, by = 0.55 / 25))

```
When we don't consider the propensity score that helps us to regulate for the large of the school, we found that this effect is very similar to the one found when we take into account this. As we can see, this ATE is concentrated between the same values as the previous one (0.2 and 0.3), holding the same hypothesis as before: there's not enough evidence to prove heterogenous effects between schools.

```{r}
#tauhat_vs_X1.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
boxplot(tau.hat ~ round(X$X1), xlab = "X1", ylab = "estimated CATE")
lines(smooth.spline(4 + X[,"X1"], tau.hat, df = 4), lwd = 2, col = 4)

```
This result is very different when we ignore the clusters. As found before. We find that the distribution of the ATE seems to hold higher values than when considering clustering, but at the same time the distribution is more disperse since now is between 0.0 a 0.6 (almost the triple size of the range!). However, as hold before, this a naive approach since it ignores the possible correlations of the student's errors that attend the same school.
```{r}
#tauhat_vs_X2.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
boxplot(tau.hat ~ round(X$X2), xlab = "X2", ylab = "estimated CATE")
lines(smooth.spline(4 + X[,"X2"], tau.hat, df = 4), lwd = 2, col = 4)

```
This plot demostrates the importance of the $X_i$ variable that is the school-level mean of students' fixed mindsets. As we can see, there seem to be a negative relation between the value of this variable and the CATE found. This explains us why is the best predictor of the measure of achievement $Y$. Nonetheless, as seen before, there's no enough evidence that this variable, when controled by other ones, is enough to prove heterogenity between the effects.
```{r}
school.avg.tauhat = t(school.mat) %*% tau.hat / school.size

#school_avg.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(school.avg.tauhat, school.pred, cex = 1.5,
     xlim = range(school.avg.tauhat, school.pred),
     ylim = range(school.avg.tauhat, school.pred),
     xlab = "average CATE estimate in school",
     ylab = "school-wise forest predictions")
abline(0, 1, lwd = 2, lty = 2, col = 4)
par = pardef

```
This graphs shows us the positive relation between the school-level predictions using school-level covariates and the average CATE estimates in school. This means that higher predictions of the measures of achievement are found when the average CATE estimate in a school is also higher.
# -------------------------------------------------------------------------------------------------------
```{r}
n.synth = 1000
p.synth = 10
X.synth = matrix(rnorm(n.synth * p.synth), n.synth, p.synth)
W.synth = rbinom(n.synth, 1, 1 / (1 + exp(-X.synth[,1])))
Y.synth = 2 * rowMeans(X.synth[,1:6]) + rnorm(n.synth)

Y.forest.synth = regression_forest(X.synth, Y.synth)
Y.hat.synth = predict(Y.forest.synth)$predictions
W.forest.synth = regression_forest(X.synth, W.synth)
W.hat.synth = predict(W.forest.synth)$predictions

cf.synth = causal_forest(X.synth, Y.synth, W.synth,
                         Y.hat = Y.hat.synth, W.hat = W.hat.synth)
ATE.synth = average_treatment_effect(cf.synth)
paste("95% CI for the ATE:", round(ATE.synth[1], 3),
      "+/-", round(qnorm(0.975) * ATE.synth[2], 3))

cf.synth.noprop = causal_forest(X.synth, Y.synth, W.synth,
                                Y.hat = Y.hat.synth, W.hat = mean(W.synth))
ATE.synth.noprop = average_treatment_effect(cf.synth.noprop)
paste("95% CI for the ATE:", round(ATE.synth.noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE.synth.noprop[2], 3))
```
# ----------------------------------------------------------------------------------------------------


# 8° Step: Visualize school-level covariates by treatment heterogeneity

```{r}
school.X.std = scale(school.X)
school.tercile = cut(school.pred,
                     breaks = c(-Inf, quantile(school.pred, c(1/3, 2/3)), Inf))
school.tercile.mat = model.matrix(~ school.tercile + 0)
school.means = diag(1 / colSums(school.tercile.mat)) %*% t(school.tercile.mat) %*% as.matrix(school.X.std)

MM = max(abs(school.means))
HC = heat.colors(21)
school.col = apply(school.means, 1:2, function(aa) HC[1 + round(20 * (0.5 + aa))])

DF.plot = data.frame(tercile=rep(factor(1:3, labels=c("low", "mid", "high")), 9), mean=as.numeric(school.means),
                     feature = factor(rbind(colnames(school.X), colnames(school.X), colnames(school.X))))

ggplot(data = DF.plot, aes(x = feature, y = tercile, fill = mean)) +
    geom_tile() + scale_fill_gradient(low = "white", high = "steelblue") +
    theme(axis.text = element_text(size=12), axis.title = element_text(size=14),
          legend.title = element_text(size=14), legend.text = element_text(size=12)) +
    theme(panel.background = element_blank())
ggsave("tercile_plot.pdf", width = 8, height = 4.5, dpi = 120)
```
RPTA: In this graph we can see the difference between the mean of the CATE for each school-level covariate considered in the school-wise estimation. We can see that there is not much hetereginty between the means in all covariates.
```{r}
mean(school.X$XC.3)
mean(school.X$XC.3[as.numeric(school.tercile) == 1])
```



# 9° Step: CATE by school 

```{r}
ord = order(order(school.pred))
school.sort = ord[school.id]

#school_boxplot.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
boxplot(tau.hat.noclust ~ school.sort, xaxt = "n",
        xlab = "school", ylab = "estimated CATE")
points(1:76, sort(school.pred), col = 4, pch = 16)
legend("topleft", c("school mean CATE", "CATE w/o clustering"), pch = c(16, 1), col = c(4, 1), cex = 1.5)
par = pardef

```

RPTA: We should first remind that these graphs are used to graph the characteristics of the heterogeneity of these groups: What happened to those groups that have a very large heterogeneous effect? And what happened to those groups that have a very small heterogenic effect?

Having mentioned that, here we have that this graph compares per-student predictions from a non-cluster-robust causal forest to per-school mean treatment effect predictions from a forest trained on per-school responses that takes clustering into account. As seen before. We can see that CATE found in when ignoring clusters is higher and more disperse by school than the one we get when considering clusters.








